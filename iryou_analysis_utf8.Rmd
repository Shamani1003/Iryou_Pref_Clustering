---
title: 都道府県別データのクラスタリング分析
date: 2018年12月
author: 西山
---

## 初期設定とデータファイルの読み込み、標準化

今回の分析作業に必要なパッケージを先に読み込んでおく。

```{r echo=T,warning=F,message=F}
library(reshape2)
library(dplyr)
library(ggplot2)
library(FactoMineR)
library(fpc)
```

OBSの授業「ビジネス統計分析」でも利用した都道府県別医療関係データを用いて、主成分分析とクラスタリング分析について発展的な事柄についてまとめたのが本資料である。データファイルはCSVファイルとして保存されている。

```{r readdata,hold=TRUE}
iryou <- read.csv(file="Iryou-Pref.csv")
head(iryou)
```

まず都道府県の名"Pref"をケース名に設定する。なおOBSの授業『ビジネス統計分析』ではRコマンダーを利用している。同じ作業をコマンドを使って行うなら以下のようになる。この資料では実行コマンドを敢えて表示して勉強の便に資したい。

```{r case_name}
rownames(iryou) <- iryou$Pref
vars <- colnames(iryou)[-1]
```

最初にデータの要約をする。量的変量なら最大値と最小値、中央値や平均値を確認しておく。質的変量なら不審なラベルがないかどうかを見ておこう。

```{r summary}
summary(iryou[,vars])
```

直ちに分かるのは項目間でデータの大きさがかなり違うという点である。主成分はばらつきの大きなデータの影響を受ける。そこでデータをすべて**標準化**して、尺度を共通化しておくのが主成分分析の鉄則である。Ｒコマンダーでは「相関行列を使う」ことで実質的には同じことをしている。

コマンド"scale"で各列のデータを標準化できるが、そのままではマトリックス型の値になる。後で主成分得点をデータセットに簡単に追加できるようにするため、iryou.stdをデータフレーム型にしている　--- Rコマンダーでデータセットと呼んでいるのはデータフレーム型の値である。

```{r normalize}
iryou.std <- as.data.frame(scale(iryou[,vars]))
```

**標準化**された数値を**標準値**と言う。この標準値は、統計分析で頻繁に使われる物差しになっている。受験でよく使われる**偏差値**も標準値から派生した尺度である。標準化をしてもデータの間の相関関係は変わらない。たとえば、身長と体重には正の相関がある―大きな人は背が高く、体重も重くなる傾向がある。実は、身長と体重を標準化しても相関係数は同じである。桁数の違いが大きいデータは扱いづらい。そんな時、単位を変えて桁数のバランスをとることが多いが、統計分析の場では標準化が最も頻繁にとられる尺度調整法になっている。

実際に、標準化されたデータの平均と分散を求めると、どの項目も平均が0、分散が1になっている。分散が1ということはその平方根である標準偏差も1である。

```{r normalizedscale}
round(apply(iryou.std,2,mean),3)
round(apply(iryou.std,2,var),3)
```

## 主成分分析

まず主成分分析を行う。Rコマンダーのメニューで主成分分析を選ぶとコマンド"princomp"が実行されるが、それよりは見やすいグラフを自動的に作成してくれるFactoMineRのPCAを使うことにする。Rコマンダーのプラグイン・ツール"RcmdrPlugin.FactoMineR"を利用しても同じである。

```{r pca}
pca <- PCA(iryou.std)
```

コマンド"PCA"を実行すると、第１主成分と第２主成分を散布図にプロットした図が自動的に作成され、プロっとされた個々の点に対応するケース名が表示されるので便利である。加えて、元の個別データと二つの主成分がどのように関連しているかも別の図に視覚化される。

図"Variables factor map (PCA)"をみると、元の個別データのほとんど全ては第１主成分に対してプラスの寄与をしていることが分かる（データD7「年間救急出動件数だけは僅かに負の寄与であるが）。第１主成分は元のデータ全体の単純平均ではないにしても、概念としては合計にかなり近いものと解釈される。

第１主成分に対して第２主成分のほうは個別データとの関連に正負の違いが出ている。大雑把にみて、D1からD14までの医療機関の数や利用状況は正の寄与を、D15より後の病院別死亡率に関するデータは負の寄与をしているようである。第２主成分は医療機関の供給状況と死亡率の状況のバランスを測っているものと先ずは解釈してよさそうである。

主成分分析のウィークポイントは、得られた主成分の意味解釈にあることは授業中に話したことではあるが、この辺は対応分析と併用するとよいかもしれない。

### 主成分得点のデータセットへの追加

FactoMineRの"PCA"コマンドは主成分得点を散布図に描いてくれるので大変便利であある。しかし、より掘り下げた分析を進めるには、やはり各都道府県の主成分得点をデータセットに追加しておきたい。それには以下のようにする。

まず主成分分析の結果を保存している"pca"の中身を確認する。

```{r look_pca}
names(pca)
```

色々な結果が保存されていることが分かるが、散布図を描くときに参照された座標値は結果"pca＄ind＄coord"の第１列"Dim.1"と第２列"Dim.2"であることが確認できる。実際、これらをそれぞれ"PC1"、"PC2"と名前をつけてデータセット"iryou.std"に追加し、散布図を描いてみると上と同じ散布図が得られる。

```{r add_pca_pc}
iryou.std$PC1 <- pca$ind$coord[,1]
iryou.std$PC2 <- pca$ind$coord[,2]
ggplot(iryou.std, aes(x=PC1,y=PC2)) +
  geom_point() +
  geom_text(aes(label=rownames(iryou.std)))
```

## クラスター分析

### クラスタリング結果のランダム性

上で得られた二つの主成分（PC1とPC2）に基づいて、都道府県別医療状況のクラスター分析を行う。

OBSの授業「ビジネス統計分析」でも説明したとおり、クラスター分析には非階層型クラスタリングと階層型クラスタリングの二つの方法がある。

ここでは授業を復習する意味合いから、まず非階層型クラスタリングを行う。非階層型クラスタリングで大切な点は、第１にクラスタ数を指定しなければならないこと。第２はクラスターの種として最初にランダムにとられる何個かの点の選び方によって結果として得られるクラスター構成に違いがもたらされる、つまり実行するたびに異なった結果になる可能性があるということである。

都道府県を幾つのクラスターに分けるかだが、散布図をみて自然にクラスター数が見て取れるわけではない。こんな場合は、まずは上下左右に分けてみるという意味合いからクラスター数を４、あるいは３と指定して、その後でクラスターごとの特徴の違いを調べるという手順をとると色々なことが分かることが多い。

```{r first_clst, echo=F}
set.seed(12345)
```
```{r cluster}
clst<-kmeans(iryou.std[,c("PC1","PC2")],centers=4,iter.max=100)
```

結果の一覧は次のとおりである。

```{r cluster.kekka}
names(clst)
length(clst$cluster)
```

結果の中の"cluster"がグループ分けされた後、各都道府県に割り当てられたグループ番号である。これを元のデータ表に"cluster"という名前で追加してから、四つのクラスターで層別化して都道府県の散布図を描く。

```{r cluster.plot}
iryou.std$cluster <- clst$cluster
p.cluster <- ggplot(iryou.std,aes(x=PC1,y=PC2,color=as.factor(cluster)))
p.cluster <- p.cluster + geom_point()
p.cluster <- p.cluster + geom_text(aes(label=rownames(iryou.std),vjust=1))
p.cluster <- p.cluster+xlim(-8.5,8.5)
p.cluster <- p.cluster+theme_bw()
plot(p.cluster)
```

クラスター分析の結果にランダム性が残っていることを確認するため同じクラスタリングをもう一度行ってみる。

```{r second_clst, echo=F}
set.seed(54321)
```

```{r cluster_agein}
clst.again <- kmeans(iryou.std[,c("PC1","PC2")],centers=4,iter.max=100)
```

２番目のクラスタリングに基づいて散布図を描く。


```{r cluster.plot.again}
iryou.std$cluster.again <- clst.again$cluster
p.cluster <- ggplot(iryou.std,aes(x=PC1,y=PC2,color=as.factor(cluster.again)))
p.cluster <- p.cluster + geom_point()
p.cluster <- p.cluster + geom_text(aes(label=rownames(iryou.std),vjust=1))
p.cluster <- p.cluster+xlim(-8.5,8.5)
p.cluster <- p.cluster+theme_bw()
plot(p.cluster)
```

１番目と２番目の散布図を見比べると、左上の区域にな東京都と大阪府が、最初のクラスタリングでは同じグループに入っているのに、２番目の散布図では別々のグループに分かれている。

各グループとクラスター番号との対応や散布図におけるクラスター番号の位置取りも二つの散布図で違いがある。

このように一度きりのクラスタリングで４７都道府県のグループ分けを確定するのは問題が残る。また、クラスター数をここではアドホックに４と指定したが、なぜクラスター数は４でなければならないか、その最適性にも説明が必要である。

### 最適なクラスター数

最適なクラスター数を決めるには、クラスター内の分散に着目する。理想的なクラスタリングは、現実には発生しないにせよ、各クラスターの内部においては全て同一の値、クラスター間にのみ違いあるという場合である。したがってクラスター内の分散が小さくなるクラスター数の方がよいと判定するのは自然である。実際、クラスター数を増やしていくと、クラスター内のデータはまとまってくる傾向がある。クラスター内の分散を合計した値が減少する速さに着目して、その減少速度が遅くなる点でクラスター数を決める方式は合理性がある。

更に、クラスター内分散に対するクラスター間分散の比率に着目してもよい。これは**Calinski-Harabasz(CH) Index**である。理想的なクラスタリングにおいては、クラスター内分散がゼロ、クラスター間でのみ分散が残るのでCH指標は無限大になる。通常のケースではなるべくCH指標がピークをとるクラスター数を選べばよい。

まず所要の関数を定義しておく。

```{r def_funcs}
sqr_edist <- function(x, y){
  sum((x-y)^2)
}

wss.cluster <- function(clustermat){
  c0 <- apply(clustermat, 2, FUN=mean)
  sum(apply(clustermat, 1, FUN=function(row){sqr_edist(row,c0)}))
}

wss.total <- function(dmatrix, labels){
  wsstot <- 0
  k <- length(unique(labels))
  for(i in 1:k){
    wsstot <- wsstot + wss.cluster(subset(dmatrix, labels==i))
  }
  wsstot
}

totss <- function(dmatrix){
  grandmean <- apply(dmatrix,2,FUN=mean)
  sum(apply(dmatrix, 1, FUN=function(row){sqr_edist(row, grandmean)}))
}
ch_criterion <- function(dmatrix, kmax, method="kmeans"){
  if(!(method %in% c("kmeans", "hclust"))){
    stop("method must be one of c('kmeans','hclust')")
  }
  npts <- dim(dmatrix)[1] # number of rows
  totss <- totss(dmatrix)
  wss <- numeric(kmax)
  crit <- numeric(kmax)
  
  wss[1] <- (npts-1)*sum(apply(dmatrix,2,var))
  for(k in 2:kmax){
    if(method=="kmeans"){
      clustering <- kmeans(dmatrix, k, nstart=10, iter.max = 100)
      wss[k] <- clustering$tot.withinss
    } else {
      d <- dist(dmatrix, method="euclidean")
      pfit <- hclust(d, method="ward")
      labels <- cutree(pfit, k=k)
      wss[k] <- wss.total(dmatrix, labels)
    }
  }
  bss <- totss - wss
  crit.num <- bss/(0:(kmax-1))
  crit.denom <- wss/(npts - 1:kmax)
  crit <- crit.num / crit.denom
  crit[1] <- NA
  list(crit=crit, wss=wss, totss=totss)
}
```

クラスター数を１個（＝クラスタリングしない）から１０個までに範囲を決め、順に主成分得点"PC1"と"PC2"に基づくkmeans"を実行しよう。このとき、クラスターごとにクラスター内分散とCH Indexを保存する。そうしてから横軸にクラスター数を、縦軸にクラスター内分散、CH Indexをとって折れ線グラフを描く。

```{r manual_calc,message=F, warning=F}
pcs <- c("PC1", "PC2")
clustcrit <- ch_criterion(iryou.std[,pcs], 10, method="kmeans")
critframe <- data.frame(k=1:10, ch=scale(clustcrit$crit), wss=scale(clustcrit$wss))
critframe2 <- melt(critframe, id.vars = c("k"), variable.name = "measure", value.name = "score")
ggplot(critframe2, aes(x=k, y=score, color=measure)) +
  geom_point(aes(shape=measure)) +
  geom_line(aes(linetype=measure)) +
  scale_x_continuous(breaks = 1:10, labels = 1:10)
```

この図をみると、クラスター数を増やすに伴ってクラスター内分散は減少している。一方、CH指標はクラスター数が３の前後でピークを形成している。６個以降はCH指標が上昇しているが、これはクラスター内分散の減少速度が鈍くなる一方でクラスター数の増加によってクラスター間の分散が増えていることよる。

最初に行ったクラスタリングではクラスター数を４としたが、最適なクラスター数としては３のほうが良いと判断できる。

### 安定したクラスタリング結果

上ではクラスタリング結果には常にランダム性が残ることに注意を促した。実際、都道府県の医療状況に着目するとしても、現実に他の都道府県と明確な違いがあり、クラスターを設けることの根拠がある現実のクラスターがある一方で、クラスター数を４と指定したことが原因となって実際には存在しないクラスターが計算上は導かれてくる、そんな見せかけのクラスターもまた得られがちである。実態の伴わないクラスタリングで形成されたグループは安定性に欠け、クラスター分析を行うたびに所属するグループが変わったりする。

パッケージ"fpc"にある"clusterboot"は何度も反復して計算をおこない最終的に安定したクラスタリングを返してくれるので大変便利である。

ここではクラスタリングを100回反復して各グループの安定性をみてみる。

```{r clustboot, message=F, results='hide'}
cboot <- clusterboot(iryou.std[,c("PC1","PC2")], clustermethod = kmeansCBI, krange=3:4)
```
最数的に確定できたグループを表示させよう。

```{r disp_final_cluster}
group <- cboot$result$partition
for(i in 1:3){
  print(paste("cluster", i))
  print(iryou$Pref[group==i])
}
```

これらのグループはクラスター計算のたびにどの程度安定して同じ構成員と組み合わされたのだろうか。

```{r look_stability_clustering}
cboot$bootmean
```

表示された数値は、いわゆる"Jaccard Coefficient"（ジャッカード係数)を100回にわたるクラスタリング計算を通して平均した値であって、ある回のクラスター計算による構成メンバーが別の回の構成メンバーとどの程度まで共通しているかという指標の平均値である。当然ながら、クラスター計算によらず同じ構成員が含まれるグループは安定性が高いわけである。上の数値をみると、どれも７５％を超えており、十分な安定性を有している。クラスターの安定性を示す平均ジャッカード係数が0.5未満になる場合は、そのクラスターに実質的な意味はないと判断される。

上の結果に基づいて層別化された散布図にしてみる。

```{r scatter_final}
iryou.std$cluster.final <- cboot$result$partition
p.cluster <- ggplot(iryou.std,aes(x=PC1,y=PC2,color=as.factor(cluster.final)))
p.cluster <- p.cluster + geom_point()
p.cluster <- p.cluster + geom_text(aes(label=rownames(iryou.std),vjust=1))
p.cluster <- p.cluster+xlim(-8.5,8.5)
p.cluster <- p.cluster+theme_bw()
plot(p.cluster)
```



<!-- ### （留意点）

上のクラスター分析から得られたクラスター別散布図は、おそらく履修者の皆がRコマンダーを用いて得たグラフとは違っていると予想する。つまり、散布図全体の形は同じであるものの、どの都道府県をどのグループに入れるかというクラスタリングが違う。そうなっているのではなかろうか。

参考までに西山がRコマンダーの主成分分析・クラスター分析から得た散布図を示しておく。Rが自動的につけるクラスター番号の違いに注意せよ。

![By R-Commander](iryou_plot_cluster.png)
-->

# 各クラスターのプロファイル分析

今回のクラスター分析の結果に基づき、各クラスターの特徴づけを行っておこう。特徴づけは標準値ではなく元データのほうが見やすいので、元のデータセット"iryou"に確定したクラスター番号を追加しておく。

```{r add_cluster_iryou}
iryou$cluster <- iryou.std$cluster.final
```

## 医療施設、医師、看護師等の数

* D1: 一般病院数（10万人当たり）

```{r summary_d1}
iryou %>% group_by(cluster) %>% summarise(avgD1=mean(D1))
```

* D2: 一般診療所数（10万人当たり）

```{r summary_d2}
iryou %>% group_by(cluster) %>% summarise(avgD2=mean(D2))
```

* D3: 一般病院病床数（10万人当たり）

```{r summary_d3}
iryou %>% group_by(cluster) %>% summarise(avgD3=mean(D3))
```

* D4: 医療施設に従事する医師数（10万人当たり）

```{r summary_d4}
iryou %>% group_by(cluster) %>% summarise(avgD4=mean(D4))
```

* D5: 医療施設に従事する（准）看護師数（10万人当たり）

```{r summary_d5}
iryou %>% group_by(cluster) %>% summarise(avgD5=mean(D5))
```

* D6: 救急自動車数（10万人当たり）

```{r summary_d6}
iryou %>% group_by(cluster) %>% summarise(avgD6=mean(D6))
```

* D7: 年間救急出動件数（千人当たり）

```{r summary_d7}
iryou %>% group_by(cluster) %>% summarise(avgD7=mean(D7))
```

* D8: 薬局数（10万人当たり）

```{r summary_d8}
iryou %>% group_by(cluster) %>% summarise(avgD8=mean(D8))
```

#### 医療サービスの供給サイドに関する情報が多いが、全体として高知・山口など地方圏から構成されるクラスターの充実が見てとれる。反対に東京など大都市圏クラスターでは人口当たりの医療施設、医師数、看護師数が相対的に低い。

## 患者数、利用率等

* D9: 一般病院外来患者数（常勤医師1人1日当たり）

```{r summary_d9}
iryou %>% group_by(cluster) %>% summarise(avgD9=mean(D9))
```

* D10: 一般病院在院患者数（常勤医師1人1日当たり）

```{r summary_d10}
iryou %>% group_by(cluster) %>% summarise(avgD10=mean(D10))
```

* D11: 一般病院在院患者数（看護師・准看護士1人1日当たり）

```{r summary_d11}
iryou %>% group_by(cluster) %>% summarise(avgD11=mean(D11))
```

* D12: 一般病院病床利用率

```{r summary_d12}
iryou %>% group_by(cluster) %>% summarise(avgD12=mean(D12))
```

* D13: 一般病院平均在院日数

```{r summary_d13}
iryou %>% group_by(cluster) %>% summarise(avgD13=mean(D13))
```

* D14: 一般病院年間新入院患者数

```{r summary_d14}
iryou %>% group_by(cluster) %>% summarise(avgD14=mean(D14))
```

#### 需要サイドの情報が多い。医師、看護師一人当たりの患者数、入院患者数をみると、大都市圏のほうが地方圏より少ない。即ち、地方圏は医療施設の供給水準は高いが、医療関係者一人当たりの患者数が多い傾向がある。これは地方圏のほうが大都市圏より高齢化が進行していることの現れである可能性もある -- 但し、平均年齢に関する情報が今回のデータには含まれていないので直ちに確認はできない。医師一人当たりの外来患者数は青森・秋田など主として寒冷地から構成されるクラスターで多い。東京など大都市圏クラスターでは平均在院日数が短いなど予想通りの結果がみられる。

## 病因別死亡率等

* D15: 標準化死亡率（千人当たり）

```{r summary_d15}
iryou %>% group_by(cluster) %>% summarise(avgD15=mean(D15))
```

* D16: 生活習慣病による死亡者数（10万人当たり）

```{r summary_d16}
iryou %>% group_by(cluster) %>% summarise(avgD16=mean(D16))
```

* D17: 悪性新生物による死亡者数（10万人当たり）

```{r summary_d17}
iryou %>% group_by(cluster) %>% summarise(avgD17=mean(D17))
```

* D18: 糖尿病による死亡者数（10万人当たり）

```{r summary_d18}
iryou %>% group_by(cluster) %>% summarise(avgD18=mean(D18))
```

* D19: 高血圧疾患による死亡者数（10万人当たり）

```{r summary_d19}
iryou %>% group_by(cluster) %>% summarise(avgD19=mean(D19))
```

* D20: 心疾患（高血圧を除く）による死亡者数（10万人当たり）

```{r summary_d20}
iryou %>% group_by(cluster) %>% summarise(avgD20=mean(D20))
```

* D21: 脳血管疾患による死亡者数（10万人当たり）

```{r summary_d21}
iryou %>% group_by(cluster) %>% summarise(avgD21=mean(D21))
```

* D22: 自殺者数（10万人当たり）

```{r summary_d22}
iryou %>% group_by(cluster) %>% summarise(avgD22=mean(D22))
```

#### 病院別の死亡率に関するデータである。挙げるべき点を列挙すると、大都市圏の人口当たり死亡者数が相対的に少ない。これも高齢化の進行状況が反映していると推測できる。また各病因について東北・北陸クラスターの死亡率が高く、全体的な標準化死亡率の相対的高さにつながっている。

### それぞれのクラスターに含まれる都道府県をみると、第１クラスターは「地方圏」、第２クラスターは「大都市型グループ」、第３クラスターは「東北・北陸ブロック」と呼べるかもしれない。限定された医療関係データによっても、これらの三つのクラスターが実際に都道府県の間の違いとして浮き彫りにされていることは興味深い。もちろん、これらの違いには違いをもたらす要因があるはずであり、これらを掘り下げていくことが今回の分析から得られるメッセージであろう。

### 階層型クラスター分析

クラスタリングには非階層型である"hclust"コマンドを用いてもよい。

階層型クラスリングでは、都道府県の間に医療状況の違いを表現される**距離**をデータから計算することから始める。その互いの距離が近い順からまず二つの都道府県を組み合わせ２個のクラスターをつくる。その後は、相互の距離が近い順からクラスターを統合し、最終的にはデータ全体を一つのクラスターにまで統合する。このプロセスにランダム性が入る余地はないので、安定的なクラスタリング結果を得るには階層型クラスター分析が適している。半面、レコード（＝１行）間の距離を求めておくことが必要なので計算負荷が大きくなり、非常に多数のケースをクラスタリングするには非階層型を用いざるを得ない。都道府県の数は４７とそれほど多くはないので、階層型方法でクラスター分析をしてもよいだろう。

まず都道府県の間の距離を求めておく。距離は**ユークリッド距離**、即ち個別データがN個あり、二つのケース$X$、$Y$の個別のデータを$X_i$、$Y_i$とおけば
$$
\sum_{i=1}^N (X_i-Y_i)^2
$$
という計算式で距離（＝違いの大きさ）を定義する。

```{r dist}
distance <- dist(iryou.std[,vars],method="euclidean")
```

この距離を"hclust"コマンドに与えると階層型クラスタリングができる。

```{r hclst, message=F, warning=F}
fit <- hclust(distance, method="ward")
plot(fit)
```

階層型クラスタリングにおいて、最初に形成される２個のクラスターから始まりデータ全体から成る１個のクラスターで終わる樹形図を**デンドログラム(dendrogram)**と呼ぶ。デンドログラムは都道府県どうしの違いを表現する距離に基づいて作成されるので、ここに結果の不確定性をもたらすランダムな要素はない。それでもデータ全体を何個のクラスターに分けるのが最適であるのかという判定はやはり難しい。

最適クラスター数の選択は階層型クラスリングにおいても同じ作業になるので、ここではクラスター数を前述のように３と設定してクラスター構造を確認しておくにとどめる。

```{r cut}
group <- cutree(fit, k=3)
for(i in 1:3){
  print(paste("cluster", i))
  print(iryou$Pref[group==i])
}
```

クラスター番号付けには違いがあるが、「大都市圏」、「地方圏」、「東北・北陸ブロック」という基本構造は共通していることに気がつくだろう。先述した非階層型クラスタリングと今回の階層型クラスタリングで異なったグループに所蔵する都道府県が見つかるとすれば、それは方法によって所属先が変わるという意味で結びつきの弱さを示すものと解釈してもよいだろう。

# 参考文献


### Zumel, N. and J. Mount "Practical Science with R", Ch.8, 2014

